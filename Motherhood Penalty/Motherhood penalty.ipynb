{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\ \\   / / | | | |   / ___/ ___|                              \n",
    "# \\ \\ / /| |_| | |   \\___ \\___ \\                              \n",
    "#  \\ V / |  _  | |___ ___) |__) |                             \n",
    "# __\\_/_ |_|_|_|_____|____/____/____  _   _  ___   ___  ____  \n",
    "#|  \\/  |/ _ \\_   _| | | | ____|  _ \\| | | |/ _ \\ / _ \\|  _ \\ \n",
    "#| |\\/| | | | || | | |_| |  _| | |_) | |_| | | | | | | | | | |\n",
    "#| |  | | |_| || | |  _  | |___|  _ <|  _  | |_| | |_| | |_| |\n",
    "#|_|__|_|\\___/_|_|_|_| |_|_____|_|_\\_\\_| |_|\\___/ \\___/|____/ \n",
    "#|  _ \\| ____| \\ | |  / \\  | | |_   _\\ \\ / /                  \n",
    "#| |_) |  _| |  \\| | / _ \\ | |   | |  \\ V /                   \n",
    "#|  __/| |___| |\\  |/ ___ \\| |___| |   | |                    \n",
    "#|_|   |_____|_| \\_/_/   \\_\\_____|_|   |_|       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'/Users/professortu/Documents/GFE/11. ATPL/VHLSS/')\n",
    "import sys\n",
    "sys.path.append(r'/Users/professortu/Documents/GFE/11. ATPL/VHLSS/')\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import linearmodels as plm\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from ha_tabulate import tab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'ttnt', 'matv', \n",
    "                   'm1ac1', 'm1ac1a', 'ttnt', 'schooling_years', 'dantoc', 'm1ac2', 'relation_to_head', 'm1ac4a', \n",
    "                   'm1ac4b', 'm1ac5', 'm4ac1a', 'm4ac2', 'm4ac6', 'm4ac7', 'm4ac8', \n",
    "                   'm4ac9', 'm4ac10b', 'total_income', 'm1ac6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is for when I merge 3 data set from 2002, 2004, 2006 together they will only have these variables left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "✦✦✦  VHLSS 2004 DATA  ✦✦✦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho1_2004                    = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2004/Data/VLSS_2004 _Ho/Ho1.dta', convert_categoricals=False)\n",
    "ho1_2004                    = ho1_2004[ho1_2004['m1c1'] != 2]\n",
    "ho1_2004['dup']             = ho1_2004.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso'], keep=False).astype(int)\n",
    "ho1_2004_cleaned            = ho1_2004[ho1_2004['dup'] == 0]\n",
    "ho1_2004_cleaned            = ho1_2004_cleaned.drop(columns=['dup'])\n",
    "ho1_2004_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']] = ho1_2004_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']].apply(pd.to_numeric, errors='coerce')\n",
    "del ho1_2004\n",
    "\n",
    "muc_123a_2004               = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2004/Data/VLSS_2004 _Ho/m1_2_3a.dta', convert_categoricals=False)\n",
    "muc_123a_2004['dup']        = muc_123a_2004.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv', 'ky'], keep=False).astype(int)\n",
    "muc_123a_2004_cleaned       = muc_123a_2004[muc_123a_2004['dup'] == 0]\n",
    "muc_123a_2004_cleaned       = muc_123a_2004_cleaned.drop(columns=['dup'])\n",
    "del muc_123a_2004\n",
    "\n",
    "muc_4a_2004                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2004/Data/VLSS_2004 _Ho/m4a.dta', convert_categoricals=False)\n",
    "muc_4a_2004['dup']          = muc_4a_2004.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv', 'ky'], keep=False).astype(int)\n",
    "muc_4a_2004_cleaned         = muc_4a_2004[muc_4a_2004['dup'] == 0]\n",
    "muc_4a_2004_cleaned         = muc_4a_2004_cleaned.drop(columns=['dup'])\n",
    "del muc_4a_2004\n",
    "\n",
    "data_2004                   = ho1_2004_cleaned.merge(muc_123a_2004_cleaned, on=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'ky'], how='inner').merge(muc_4a_2004_cleaned, on=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], how='left')\n",
    "\n",
    "data_2004.rename(columns={'m2c1': 'schooling_years'}, inplace=True)\n",
    "data_2004.rename(columns={'m1ac3': 'relation_to_head'}, inplace=True)\n",
    "\n",
    "data_2004['m4ac11']         = pd.to_numeric(data_2004['m4ac11'], errors='coerce')\n",
    "data_2004['m4ac12e']        = pd.to_numeric(data_2004['m4ac12e'], errors='coerce')\n",
    "data_2004['m4ac21']         = pd.to_numeric(data_2004['m4ac21'], errors='coerce')\n",
    "data_2004['m4ac22e']        = pd.to_numeric(data_2004['m4ac22e'], errors='coerce')\n",
    "data_2004['m4ac25']         = pd.to_numeric(data_2004['m4ac25'], errors='coerce')\n",
    "\n",
    "data_2004['total_income']   = data_2004[['m4ac11', 'm4ac12e', 'm4ac21', 'm4ac22e', 'm4ac25']].sum(axis=1, skipna=True)\n",
    "\n",
    "data_2004                   = data_2004[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✦✦✦  VHLSS 2006 DATA  ✦✦✦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/p42kz6cs7fs357mxprjc7kq40000gn/T/ipykernel_21292/642075841.py:13: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  muc_2a_2006                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2006/Data/2006_Ho/Income/muc2a.dta', convert_categoricals=False)\n",
      "/var/folders/lm/p42kz6cs7fs357mxprjc7kq40000gn/T/ipykernel_21292/642075841.py:27: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  muc_4a_2006                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2006/Data/2006_Ho/Income/muc4a.dta', convert_categoricals=False)\n"
     ]
    }
   ],
   "source": [
    "ttchung_2006                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2006/Data/2006_Ho/Income/ttchung.dta', convert_categoricals=False)\n",
    "ttchung_2006                 = ttchung_2006.drop(columns=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'ttnt'])\n",
    "columns_to_rename            = ['tinh04', 'huyen04', 'xa04', 'diaban04', 'hoso04', 'ttnt04']\n",
    "columns_to_move              = ['tinh', 'huyen', 'xa', 'diaban', 'hoso','ttnt']\n",
    "ttchung_2006                 = ttchung_2006.rename(columns={col: col[:-2] for col in columns_to_rename})\n",
    "ttchung_2006                 = ttchung_2006.reindex(columns=columns_to_move + [col for col in ttchung_2006.columns if col not in columns_to_move])\n",
    "ttchung_2006['dup']          = ttchung_2006.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso'], keep=False).astype(int)\n",
    "ttchung_2006_cleaned         = ttchung_2006[ttchung_2006['dup'] == 0]\n",
    "ttchung_2006_cleaned         = ttchung_2006_cleaned.drop(columns=['dup'])\n",
    "ttchung_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']] = ttchung_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']].apply(pd.to_numeric, errors='coerce')\n",
    "del columns_to_move, columns_to_rename, ttchung_2006\n",
    "\n",
    "muc_2a_2006                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2006/Data/2006_Ho/Income/muc2a.dta', convert_categoricals=False)\n",
    "muc_2a_2006['dup']          = muc_2a_2006.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], keep=False).astype(int)\n",
    "muc_2a_2006_cleaned         = muc_2a_2006[muc_2a_2006['dup'] == 0]\n",
    "muc_2a_2006_cleaned         = muc_2a_2006_cleaned.drop(columns=['dup'])\n",
    "muc_2a_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']] = muc_2a_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']].apply(pd.to_numeric, errors='coerce')\n",
    "del muc_2a_2006\n",
    "\n",
    "muc_1a_2006                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2006/Data/2006_Ho/Income/muc1a.dta', convert_categoricals=False)\n",
    "muc_1a_2006['dup']          = muc_1a_2006.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], keep=False).astype(int)\n",
    "muc_1a_2006_cleaned         = muc_1a_2006[muc_1a_2006['dup'] == 0]\n",
    "muc_1a_2006_cleaned         = muc_1a_2006_cleaned.drop(columns=['dup'])\n",
    "muc_1a_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']] = muc_1a_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']].apply(pd.to_numeric, errors='coerce')\n",
    "del muc_1a_2006\n",
    "\n",
    "muc_4a_2006                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2006/Data/2006_Ho/Income/muc4a.dta', convert_categoricals=False)\n",
    "muc_4a_2006['dup']          = muc_4a_2006.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], keep=False).astype(int)\n",
    "muc_4a_2006_cleaned         = muc_4a_2006[muc_4a_2006['dup'] == 0]\n",
    "muc_4a_2006_cleaned         = muc_4a_2006_cleaned.drop(columns=['dup'])\n",
    "muc_4a_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']] = muc_4a_2006_cleaned[['tinh', 'huyen', 'xa', 'diaban', 'hoso']].apply(pd.to_numeric, errors='coerce')\n",
    "del muc_4a_2006\n",
    "\n",
    "data_2006                   = (  ttchung_2006_cleaned            \n",
    "                               .merge(muc_1a_2006_cleaned, on=['tinh', 'huyen', 'xa', 'diaban', 'hoso'], how='inner')\n",
    "                               .merge(muc_2a_2006_cleaned, on=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], how='left')\n",
    "                               .merge(muc_4a_2006_cleaned, on=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], how='left')     \n",
    "                               )\n",
    "\n",
    "\n",
    "data_2006['m4ac11']         = pd.to_numeric(data_2006['m4ac11'], errors='coerce')\n",
    "data_2006['m4ac12f']        = pd.to_numeric(data_2006['m4ac12f'], errors='coerce')\n",
    "data_2006['m4ac21']         = pd.to_numeric(data_2006['m4ac21'], errors='coerce')\n",
    "data_2006['m4ac22f']        = pd.to_numeric(data_2006['m4ac22f'], errors='coerce')\n",
    "data_2006['m4ac25']         = pd.to_numeric(data_2006['m4ac25'], errors='coerce')\n",
    "\n",
    "data_2006['total_income'] = data_2006[['m4ac11', 'm4ac12f', 'm4ac21', 'm4ac22f', 'm4ac25']].sum(axis=1, skipna=True)\n",
    "\n",
    "data_2006.rename(columns={'m2ac1': 'schooling_years'}, inplace=True)\n",
    "data_2006.rename(columns={'m1ac3': 'relation_to_head'}, inplace=True)\n",
    "\n",
    "data_2006                   = data_2006[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✦✦✦  VHLSS 2008 DATA  ✦✦✦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_2008                      = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2008/Data/2008_ Ho/ho.dta', convert_categoricals=False)\n",
    "ho_2008                      = ho_2008.drop(columns=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'ttnt'])\n",
    "columns_to_rename            = ['tinh06', 'huyen06', 'xa06', 'diaban06', 'hoso06', 'ttnt06']\n",
    "columns_to_move              = ['tinh', 'huyen', 'xa', 'diaban', 'hoso','ttnt']\n",
    "ho_2008                      = ho_2008.rename(columns={col: col[:-2] for col in columns_to_rename})\n",
    "ho_2008                      = ho_2008.reindex(columns=columns_to_move + [col for col in ho_2008.columns if col not in columns_to_move])\n",
    "ho_2008['dup']               = ho_2008.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso'], keep=False).astype(int)\n",
    "ho_2008_cleaned              = ho_2008[ho_2008['dup'] == 0]\n",
    "ho_2008_cleaned              = ho_2008_cleaned.drop(columns=['dup'])\n",
    "del columns_to_move,columns_to_rename, ho_2008\n",
    "\n",
    "ho11_2008                     = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2008/Data/2008_ Ho/ho11.dta', convert_categoricals=False)\n",
    "ho11_2008['dup']              = ho11_2008.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso'], keep=False).astype(int)\n",
    "ho11_2008_cleaned             = ho11_2008[ho11_2008['dup'] == 0]\n",
    "ho11_2008_cleaned             = ho11_2008_cleaned.drop(columns=['dup'])\n",
    "del ho11_2008\n",
    "\n",
    "muc_123a_2008               = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2008/Data/2008_ Ho/muc123a_ori.dta', convert_categoricals=False)\n",
    "muc_123a_2008['dup']        = muc_123a_2008.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], keep=False).astype(int)\n",
    "muc_123a_2008_cleaned       = muc_123a_2008[muc_123a_2008['dup'] == 0]\n",
    "muc_123a_2008_cleaned       = muc_123a_2008_cleaned.drop(columns=['dup'])\n",
    "del muc_123a_2008\n",
    "\n",
    "\n",
    "muc_4a_2008                 = pd.read_stata('/Users/professortu/Documents/GFE/11. ATPL/VHLSS/2008/Data/2008_ Ho/muc4a.dta', convert_categoricals=False)\n",
    "muc_4a_2008['dup']          = muc_4a_2008.duplicated(subset=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], keep=False).astype(int)\n",
    "muc_4a_2008_cleaned         = muc_4a_2008[muc_4a_2008['dup'] == 0]\n",
    "muc_4a_2008_cleaned         = muc_4a_2008_cleaned.drop(columns=['dup'])\n",
    "del muc_4a_2008\n",
    "\n",
    "data_2008                   = (ho_2008_cleaned  \n",
    "                               .merge(muc_123a_2008_cleaned, on=['tinh', 'huyen', 'xa', 'diaban', 'hoso'], how='inner')\n",
    "                               .merge(muc_4a_2008_cleaned, on=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'], how='left')\n",
    "                               )\n",
    "\n",
    "data_2008['m4ac11']         = pd.to_numeric(data_2008['m4ac11'], errors='coerce')\n",
    "data_2008['m4ac12f']        = pd.to_numeric(data_2008['m4ac12f'], errors='coerce')\n",
    "data_2008['m4ac21']         = pd.to_numeric(data_2008['m4ac21'], errors='coerce')\n",
    "data_2008['m4ac22f']        = pd.to_numeric(data_2008['m4ac22f'], errors='coerce')\n",
    "data_2008['m4ac25']         = pd.to_numeric(data_2008['m4ac25'], errors='coerce')\n",
    "\n",
    "\n",
    "data_2008['total_income']   = data_2008[['m4ac11', 'm4ac12f', 'm4ac21', 'm4ac22f', 'm4ac25']].sum(axis=1, skipna=True)\n",
    "\n",
    "data_2008.rename(columns={'m2ac1': 'schooling_years'}, inplace=True)\n",
    "data_2008.rename(columns={'m1ac3': 'relation_to_head'}, inplace=True)\n",
    "\n",
    "data_2008                   = data_2008[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✦✦✦  PANEL DATA  ✦✦✦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2004['year']            = 2004\n",
    "data_2006['year']            = 2006\n",
    "data_2008['year']            = 2008\n",
    "\n",
    "panel_data                   = pd.concat([data_2004, data_2006, data_2008], axis=0)\n",
    "panel_data.sort_values(by=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv', 'year'], inplace=True)\n",
    "panel_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "panel_data['ethnic']        = panel_data['dantoc'].apply(lambda x: 1 if x == 1 else 0)\n",
    "panel_data['marital_status'] = panel_data['m1ac6'].apply(lambda x: 1 if x==2 else 0)\n",
    "panel_data['working?']       = panel_data['m4ac1a'].apply(lambda x: 1 if x == 1 else 0)\n",
    "panel_data['canbo?']         = panel_data['m4ac10b'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "\n",
    "zero_income_rows = panel_data[panel_data['total_income'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✦✦✦  MOTHER OF SMALL CHILD  ✦✦✦ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "females_panel               = panel_data[panel_data['m1ac2'] == 2]\n",
    "children_panel              = panel_data[panel_data['m1ac5'] < 3]\n",
    "mothers_head_wife           = females_panel[females_panel['relation_to_head'].isin([1, 2])]\n",
    "mothers_daughter            = females_panel[females_panel['relation_to_head'] == 3]\n",
    "matched_head_wife           = pd.merge(children_panel[children_panel['relation_to_head'] == 3],\n",
    "                              mothers_head_wife,\n",
    "                              on=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'year'],\n",
    "                              suffixes=('_child', '_mother'))\n",
    "matched_daughters           = pd.merge(children_panel[children_panel['relation_to_head'] == 6],\n",
    "                              mothers_daughter,\n",
    "                              on=['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'year'],\n",
    "                              suffixes=('_child', '_mother'))\n",
    "all_mothers_panel           = pd.concat([matched_head_wife, matched_daughters])\n",
    "\n",
    "panel_data['mother_of_small_child'] = 0\n",
    "panel_data.loc[panel_data[['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv', 'year']].apply(tuple, axis=1).isin(\n",
    "    all_mothers_panel[['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv_mother', 'year']].apply(tuple, axis=1)),\n",
    "    'mother_of_small_child'] = 1\n",
    "\n",
    "del females_panel, children_panel, mothers_head_wife, mothers_daughter, matched_head_wife, matched_daughters, all_mothers_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✦✦✦ DROPPING NAN AND CLEANNING THE PANEL ✦✦✦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_data                  = panel_data.dropna()\n",
    "panel_data                  = panel_data[panel_data['total_income'] > 0]\n",
    "panel_data                  = panel_data.drop(columns=['dantoc','m1ac2', 'm1ac6', 'm4ac1a', 'm4ac10b', 'relation_to_head'])\n",
    "\n",
    "\n",
    "unique_years_per_individual = panel_data.groupby(['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'])['year'].nunique()\n",
    "valid_individuals           = unique_years_per_individual[unique_years_per_individual > 1].index\n",
    "panel_data                  = panel_data[panel_data.set_index(['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv']).index.isin(valid_individuals)]\n",
    "panel_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "year_pairs                  = [(2004, 2006), (2006, 2008)]\n",
    "grouped                     = panel_data.groupby(['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv'])\n",
    "panel_data                  = grouped.filter(lambda x: len(x['year'].unique()) == 2 and tuple(sorted(x['year'].unique())) in year_pairs)\n",
    "del valid_individuals, unique_years_per_individual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ✦✦✦ RENAME AND ADDING CONTROL VARIABLES ✦✦✦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_data.rename(columns={'m1ac5': 'age'}, inplace=True)\n",
    "panel_data.rename(columns={'m1ac2': 'gender'}, inplace=True)\n",
    "panel_data['age_sq']        = panel_data['age']**2\n",
    "panel_data['log_income']    = np.log(panel_data['total_income'])\n",
    "panel_data['id']            = panel_data.groupby(['tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv']).ngroup() + 1 \n",
    "\n",
    "columns_order               = [\n",
    "                                'tinh', 'huyen', 'xa', 'diaban', 'hoso', 'matv', 'm1ac1', 'm1ac1a', 'year', 'id',\n",
    "                                'mother_of_small_child', 'log_income', 'age', 'age_sq', 'schooling_years', 'ttnt', \n",
    "                                'ethnic', 'marital_status', 'working?', 'canbo?', 'total_income'\n",
    "                                    ]\n",
    "panel_data                  = panel_data[columns_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✦✦✦ MODEL: FIRST DIFFERENCE ✦✦✦ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X                           = panel_data[['mother_of_small_child', 'age', 'age_sq', 'schooling_years', 'ttnt', 'ethnic',\n",
    "                                          'marital_status', 'working?', 'canbo?']]\n",
    "Y                           = panel_data['log_income']\n",
    "\n",
    "first_diff                  = pd.concat([panel_data['id'], X], axis=1)\n",
    "first_diff                  = first_diff.groupby('id').diff()\n",
    "\n",
    "first_diff.dropna(inplace=True)\n",
    "first_diff.reset_index(drop=True, inplace=True)\n",
    "first_diff = first_diff.loc[:, (first_diff != 0).any(axis=0)]\n",
    "del first_diff\n",
    "\n",
    "X_after                     = panel_data[['mother_of_small_child', 'age', 'age_sq', 'schooling_years', 'ethnic',\n",
    "                                          'marital_status', 'canbo?']]\n",
    "\n",
    "panel_data.set_index(['id', 'year'], drop=False, inplace=True)\n",
    "\n",
    "firstdf_regress = plm.FirstDifferenceOLS.from_formula(\n",
    "    formula='log_income ~ mother_of_small_child + age + age_sq + ttnt + schooling_years + ethnic + marital_status + `canbo?`',\n",
    "    data=panel_data\n",
    ")\n",
    "results_fd                 = firstdf_regress.fit(cov_type='clustered',cluster_entity=True)\n",
    "table_fd                   = pd.DataFrame({\n",
    "                                'b': round(results_fd.params, 4),\n",
    "                                'se': round(results_fd.std_errors, 4),\n",
    "                                't': round(results_fd.tstats, 4),\n",
    "                                'pval': round(results_fd.pvalues, 4)\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            b      se       t    pval\n",
      "mother_of_small_child -0.0150  0.0635 -0.2368  0.8128\n",
      "age                    0.0980  0.0431  2.2760  0.0230\n",
      "age_sq                 0.0001  0.0005  0.1720  0.8634\n",
      "ttnt[ttnt]            -0.0457  0.1063 -0.4301  0.6672\n",
      "schooling_years        0.0037  0.0180  0.2078  0.8354\n",
      "ethnic                -0.0276  0.1022 -0.2705  0.7868\n",
      "marital_status         0.1353  0.0901  1.5014  0.1334\n",
      "canbo?                 0.1940  0.0402  4.8241  0.0000\n"
     ]
    }
   ],
   "source": [
    "print(table_fd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
